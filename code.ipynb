{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3h8c8K20V6c"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os # 'os' is needed for script logic, so we import it here.\n",
        "\n",
        "def install_and_import(package, import_name=None):\n",
        "    \"\"\"\n",
        "    Tries to import a package. If it fails, attempts to install it via pip\n",
        "    and then tries to import it again.\n",
        "    \"\"\"\n",
        "    if import_name is None:\n",
        "        import_name = package\n",
        "\n",
        "    try:\n",
        "        __import__(import_name)\n",
        "        print(f\"{import_name} is already installed.\")\n",
        "    except ImportError:\n",
        "        print(f\"{import_name} not found. Attempting to install {package}...\")\n",
        "        try:\n",
        "            # Use sys.executable to ensure pip is called for the correct python env\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "            print(f\"Successfully installed {package}.\")\n",
        "            __import__(import_name)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: Failed to install {package}. {e}\")\n",
        "            print(f\"Please install {package} manually by running: pip install {package}\")\n",
        "            sys.exit(1)\n",
        "\n",
        "def setup_dependencies():\n",
        "    \"\"\"\n",
        "    Checks and installs all required dependencies.\n",
        "    \"\"\"\n",
        "    print(\"Checking dependencies...\")\n",
        "    install_and_import(\"torch\")\n",
        "    install_and_import(\"numpy\", \"numpy\")\n",
        "    install_and_import(\"einops\", \"einops\")\n",
        "    install_and_import(\"Pillow\", \"PIL\")\n",
        "    install_and_import(\"torchvision\", \"torchvision\")\n",
        "    print(\"All dependencies are set up.\")\n",
        "\n",
        "# --- Run Dependency Setup ---\n",
        "# This block will run when the script is executed.\n",
        "# It ensures all required packages are installed before they are imported.\n",
        "setup_dependencies()\n",
        "\n",
        "# --- Main Script Imports ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import time\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import glob\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# --- Helper Functions & Classes ---\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    A common loss function for segmentation tasks.\n",
        "    It measures the overlap between the predicted mask and the ground truth.\n",
        "    \"\"\"\n",
        "    def __init__(self, smooth=1.0):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Apply sigmoid to get probabilities\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "\n",
        "        # Flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\n",
        "\n",
        "        return 1 - dice\n",
        "\n",
        "def dice_coefficient(inputs, targets, smooth=1.0):\n",
        "    \"\"\"\n",
        "    Calculates the Dice Coefficient (a metric, not a loss)\n",
        "    \"\"\"\n",
        "    # Apply sigmoid and threshold to get binary predictions\n",
        "    inputs = torch.sigmoid(inputs)\n",
        "    inputs = (inputs > 0.5).float()\n",
        "\n",
        "    inputs = inputs.view(-1)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    intersection = (inputs * targets).sum()\n",
        "    dice = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
        "\n",
        "    return dice.item()\n",
        "\n",
        "# --- U-Net Model Architecture ---\n",
        "#\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(Convolution => BatchNorm => ReLU) * 2\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with MaxPool then DoubleConv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then DoubleConv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "        # if bilinear, use the normal interpolation to reduce artifacts\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # x2 is the skip connection\n",
        "        # Concatenate along the channel dimension\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard U-Net implementation.\n",
        "    The number of output channels is 1 (for a binary mask).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_channels=1, n_classes=1, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits\n",
        "\n",
        "# --- Vision Transformer (ViT) Segmentation Model ---\n",
        "#\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Splits the image into patches and embeds them.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=128, patch_size=16, in_channels=1, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        # This layer does the patching and embedding in one go\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels, embed_dim,\n",
        "            kernel_size=patch_size, stride=patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # (B, E, H_p, W_p)\n",
        "        x = x.flatten(2)  # (B, E, N_p)\n",
        "        x = x.transpose(1, 2)  # (B, N_p, E)\n",
        "        return x\n",
        "\n",
        "class ViTSeg(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified Vision Transformer for Segmentation.\n",
        "    This model uses a Transformer Encoder followed by a simple\n",
        "    convolutional decoder to upscale the features back to image size.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=128, patch_size=16, in_channels=1, n_classes=1,\n",
        "                 embed_dim=768, depth=12, n_heads=12, mlp_dim=3072):\n",
        "        super(ViTSeg, self).__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        # 1. Patch Embedding\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "\n",
        "        # 2. Positional Embedding\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches, embed_dim))\n",
        "\n",
        "        # 3. Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim, nhead=n_heads, dim_feedforward=mlp_dim, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "\n",
        "        # 4. Simple Convolutional Decoder\n",
        "        # Reshape (B, N_p, E) -> (B, E, H_p, W_p)\n",
        "        self.patch_h = img_size // patch_size\n",
        "        self.patch_w = img_size // patch_size\n",
        "\n",
        "        # Use ConvTranspose2d to upsample\n",
        "        # We need to upsample by a factor of `patch_size` (e.g., 16)\n",
        "        # We can do this in steps (e.g., 4x, then 4x)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(embed_dim, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "        )\n",
        "        # Final conv to get to the number of classes\n",
        "        self.out_conv = nn.Conv2d(32, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Patch Embedding\n",
        "        x = self.patch_embed(x)  # (B, N_p, E)\n",
        "\n",
        "        # 2. Add Positional Embedding\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # 3. Transformer Encoder\n",
        "        x = self.transformer_encoder(x)  # (B, N_p, E)\n",
        "\n",
        "        # 4. Decoder\n",
        "        # Reshape for decoder: (B, N_p, E) -> (B, E, H_p, W_p)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = x.view(x.shape[0], self.embed_dim, self.patch_h, self.patch_w)\n",
        "\n",
        "        # Upsample\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        # Final output conv\n",
        "        logits = self.out_conv(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "# --- Data Download and Loading ---\n",
        "\n",
        "def download_and_extract_dataset():\n",
        "    \"\"\"\n",
        "    Downloads and extracts the 2D brain tumor segmentation dataset from Zenodo.\n",
        "    Uses Zenodo record 12735702 as a reliable source.\n",
        "    \"\"\"\n",
        "    # *** NEW, WORKING DATASET URL ***\n",
        "    dataset_url = \"https://zenodo.org/records/12735702/files/brain-tumor-mri-dataset.zip?download=1\"\n",
        "    data_dir = \"./\"\n",
        "    zip_path = os.path.join(data_dir, \"brain-tumor-mri-dataset.zip\")\n",
        "    # This is the folder name *inside* the zip file\n",
        "    extract_path = os.path.join(data_dir, \"brain-tumor-mri-dataset\")\n",
        "\n",
        "    if os.path.exists(extract_path):\n",
        "        print(f\"Dataset already found at {extract_path}\")\n",
        "        return extract_path\n",
        "\n",
        "    print(\"Downloading dataset (156MB)... (This may take a few minutes)\")\n",
        "    try:\n",
        "        # Use a context manager to ensure the request is closed\n",
        "        with urllib.request.urlopen(dataset_url) as response:\n",
        "            # Check if response is successful\n",
        "            if response.status != 200:\n",
        "                print(f\"Error downloading dataset: HTTP Status {response.status}\")\n",
        "                return None\n",
        "\n",
        "            with open(zip_path, 'wb') as out_file:\n",
        "                out_file.write(response.read())\n",
        "        print(\"Download complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading dataset: {e}\")\n",
        "        # Clean up partial download if it exists\n",
        "        if os.path.exists(zip_path):\n",
        "            os.remove(zip_path)\n",
        "        return None\n",
        "\n",
        "    print(\"Extracting dataset...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(data_dir)\n",
        "        print(\"Extraction complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Clean up the zip file\n",
        "    try:\n",
        "        os.remove(zip_path)\n",
        "        print(f\"Removed zip file: {zip_path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error removing zip file: {e}\")\n",
        "\n",
        "    return extract_path\n",
        "\n",
        "# We need a custom dataset class to apply different transforms\n",
        "# to images (normalize) and masks (don't normalize)\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Loads the downloaded 2D MRI dataset from Zenodo record 12735702.\n",
        "    Applies separate transforms for images and masks.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dir, image_transform=None, mask_transform=None):\n",
        "        # *** UPDATED PATHS to match the new zip file's structure ***\n",
        "        image_dir = os.path.join(data_dir, \"images\")\n",
        "        mask_dir = os.path.join(data_dir, \"masks\")\n",
        "\n",
        "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.png\")))\n",
        "        self.mask_paths = sorted(glob.glob(os.path.join(mask_dir, \"*.png\")))\n",
        "        self.image_transform = image_transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "        # Sanity check\n",
        "        if not self.image_paths or not self.mask_paths:\n",
        "            print(f\"Error: No images or masks found in {data_dir}. Check directory structure.\")\n",
        "            print(f\"Looking in: {image_dir} and {mask_dir}\")\n",
        "            raise FileNotFoundError(f\"Dataset files not found in {image_dir} or {mask_dir}\")\n",
        "\n",
        "        assert len(self.image_paths) == len(self.mask_paths), \\\n",
        "            \"Number of images and masks do not match!\"\n",
        "        print(f\"Found {len(self.image_paths)} image/mask pairs.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        mask_path = self.mask_paths[idx]\n",
        "\n",
        "        try:\n",
        "            # Load image and mask\n",
        "            image = Image.open(image_path).convert(\"L\")  # Convert to grayscale\n",
        "            mask = Image.open(mask_path).convert(\"L\")   # Convert to grayscale\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image/mask at index {idx}: {e}\")\n",
        "            print(f\"Image path: {image_path}, Mask path: {mask_path}\")\n",
        "            # Return a dummy tensor to avoid crashing the loader\n",
        "            return torch.zeros(1, 128, 128), torch.zeros(1, 128, 128)\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.image_transform:\n",
        "            image = self.image_transform(image)\n",
        "        if self.mask_transform:\n",
        "            mask = self.mask_transform(mask)\n",
        "\n",
        "        # Binarize mask (some masks might have anti-aliasing)\n",
        "        mask = (mask > 0.5).float()\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# --- Training and Evaluation Logic ---\n",
        "\n",
        "def train_model(model, dataloader, criterion, optimizer, device, num_epochs=3):\n",
        "    \"\"\"\n",
        "    A simple training loop.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start = time.time()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, (images, masks) in enumerate(dataloader):\n",
        "            # Check for dummy data from loader errors\n",
        "            if images.shape[0] == 0:\n",
        "                continue\n",
        "\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if (i + 1) % 20 == 0 or i == len(dataloader) - 1:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Avg. Loss: {epoch_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
        "\n",
        "    print(\"Finished Training.\")\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    A simple evaluation loop to calculate the average Dice Coefficient.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    total_dice = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in dataloader:\n",
        "            if images.shape[0] == 0:\n",
        "                continue\n",
        "\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            dice = dice_coefficient(outputs, masks)\n",
        "            total_dice += dice\n",
        "\n",
        "    # Handle empty dataloader case\n",
        "    if len(dataloader) == 0:\n",
        "        print(\"Warning: Empty validation dataloader.\")\n",
        "        return 0.0\n",
        "\n",
        "    avg_dice = total_dice / len(dataloader)\n",
        "    return avg_dice\n",
        "\n",
        "# --- Main Comparison Function ---\n",
        "\n",
        "def main():\n",
        "    # --- 1. Configuration ---\n",
        "    # Hyperparameters\n",
        "    NUM_EPOCHS = 3\n",
        "    BATCH_SIZE = 8\n",
        "    LEARNING_RATE = 1e-4\n",
        "    IMG_SIZE = 128\n",
        "\n",
        "    # ViT specific parameters\n",
        "    # Note: These are small to make the model runnable on most systems.\n",
        "    # Real ViTs are much larger (e.g., embed_dim=768, depth=12)\n",
        "    VIT_PATCH_SIZE = 16\n",
        "    VIT_EMBED_DIM = 256  # Reduced from 768 for demo\n",
        "    VIT_DEPTH = 4        # Reduced from 12 for demo\n",
        "    VIT_HEADS = 8        # Reduced from 12 for demo\n",
        "    VIT_MLP_DIM = 1024   # Reduced from 3072 for demo\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # --- 2. Data ---\n",
        "    print(\"Checking for dataset...\")\n",
        "    data_dir = download_and_extract_dataset()\n",
        "    if data_dir is None:\n",
        "        print(\"Failed to get dataset. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Define transformations\n",
        "    # We normalize images to [-1, 1] range for better model stability\n",
        "    data_transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "    ])\n",
        "\n",
        "    # Masks should just be resized and converted to a tensor\n",
        "    mask_transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # Create full dataset\n",
        "    try:\n",
        "        full_dataset = CustomDataset(data_dir, image_transform=data_transform, mask_transform=mask_transform)\n",
        "    except FileNotFoundError as e:\n",
        "        print(e)\n",
        "        return\n",
        "\n",
        "    if len(full_dataset) == 0:\n",
        "        print(\"Dataset loaded 0 samples. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Split into training and validation\n",
        "    # Using a simple 80/20 split\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "\n",
        "    # Check if dataset is large enough\n",
        "    if train_size == 0 or val_size == 0:\n",
        "        print(\"Dataset is too small to split. Using all data for training.\")\n",
        "        train_dataset = full_dataset\n",
        "        val_dataset = full_dataset # Not ideal, but prevents crash\n",
        "    else:\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    print(f\"Training on {len(train_dataset)} samples, validating on {len(val_dataset)} samples.\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # --- 3. Models ---\n",
        "    print(\"Initializing models...\")\n",
        "    # U-Net\n",
        "    model_unet = UNet(n_channels=1, n_classes=1)\n",
        "\n",
        "    # Vision Transformer\n",
        "    model_vit = ViTSeg(\n",
        "        img_size=IMG_SIZE,\n",
        "        patch_size=VIT_PATCH_SIZE,\n",
        "        embed_dim=VIT_EMBED_DIM,\n",
        "        depth=VIT_DEPTH,\n",
        "        n_heads=VIT_HEADS,\n",
        "        mlp_dim=VIT_MLP_DIM\n",
        "    )\n",
        "\n",
        "    # --- 4. Loss and Optimizers ---\n",
        "    criterion = DiceLoss()\n",
        "    optimizer_unet = optim.Adam(model_unet.parameters(), lr=LEARNING_RATE)\n",
        "    optimizer_vit = optim.Adam(model_vit.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # --- 5. Train and Evaluate U-Net ---\n",
        "    print(\"\\n--- Training U-Net ---\")\n",
        "    start_unet = time.time()\n",
        "    model_unet = train_model(model_unet, train_loader, criterion, optimizer_unet, device, num_epochs=NUM_EPOCHS)\n",
        "    time_unet = time.time() - start_unet\n",
        "\n",
        "    print(\"Evaluating U-Net...\")\n",
        "    dice_unet = evaluate_model(model_unet, val_loader, device)\n",
        "\n",
        "    # --- 6. Train and Evaluate ViT ---\n",
        "    print(\"\\n--- Training Vision Transformer (ViT-Seg) ---\")\n",
        "    start_vit = time.time()\n",
        "    model_vit = train_model(model_vit, train_loader, criterion, optimizer_vit, device, num_epochs=NUM_EPOCHS)\n",
        "    time_vit = time.time() - start_vit\n",
        "\n",
        "    print(\"Evaluating ViT-Seg...\")\n",
        "    dice_vit = evaluate_model(model_vit, val_loader, device)\n",
        "\n",
        "    # --- 7. Comparison ---\n",
        "    print(\"\\n--- Comparison Results ---\")\n",
        "    print(f\"Trained on {len(train_dataset)} images for {NUM_EPOCHS} epochs.\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"| Model     | Avg. Val Dice Score | Training Time |\")\n",
        "    print(f\"|-----------|---------------------|---------------|\")\n",
        "    print(f\"| U-Net     | {dice_unet:.4f}             | {time_unet:.2f}s        |\")\n",
        "    print(f\"| ViT-Seg   | {dice_vit:.4f}             | {time_vit:.2f}s        |\")\n",
        "    print(\"-\" * 30)\n",
        "    print(\"\\nNote: These results are from a short training run on small models.\")\n",
        "    print(\"For a real study, you would need more epochs, larger models,\")\n",
        "    print(\"and rigorous hyperparameter tuning.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}